---
layout: post
read_time: true
show_date: true
title: Evidential Deep Learning
date: 2022-12-12
img: posts/20221212/back.png
tags: [证据学习, 论文笔记]
author: 李哲
description: 论文《Evidential Deep Learning》的阅读笔记
---

### 问题
- 由于标准方法是训练网络以使预测损失最小化，模型对其预测置信度一无所知。
- 已有的**方式：**通过权重不确定性间接推断预测不确定性的贝叶斯神经网络
- **提出：**使用主观逻辑理论（SL）的显式建模。在类概率上放置狄利克雷分布，将神经网络的预测视为主观意见。学习从数据中收集导致这些意见的确定性神经网络的证据的函数。
- **Softmax存在的问题：**夸大预测类别的概率，结果是不可靠的不确定性估计。（预测的标签的距离除了与其他类别的比较值外，对结论没有用处）
![举例](./assets/img/posts/20221212/cite.png)

### 理论说明
- **Subjective Logic (SL)主观逻辑**，为每个单例提供 可信度 bk 考虑 K 个互斥单例(类标签)，并提供总体不确定性u。

  ![Untitled](./assets/img/posts/20221212/1.png)

    - 可信度 bk 由单例的证据 ek 计算。设 ek≥0 为第 k 个单例的证据，可信度 bk 和不确定性 u 计算为

      ![Untitled](./assets/img/posts/20221212/2.png)

      ![Untitled](./assets/img/posts/20221212/3.png)

    - 不确定性与总证据成反比
        - 当没有证据时，每个单例的信度为0，不确定性为1。
        - **证据**是从数据中收集的支持量的度量，用以支持将样本分类到某个类别。
        - **可信度**（主观意见），对应于 参数αk = ek + 1的狄利克雷分布
        - bk = (αk−1)/S，从相应的狄利克雷分布的参数中得到主观意见，S =∑k αi被称为狄利克雷强度
        - ![Untitled](./assets/img/posts/20221212/4.png)

        - ![Untitled](./assets/img/posts/20221212/5.png)
- 举例
    - 假设b = < 0，…， 0 >为10类分类问题的可信度分配。则对图像进行分类的先验分布变为均匀分布，即D(p; < 1，…， 1 >) 参数均为1的狄利克雷分布。简单说就是因为没有观察到的证据，所以可信度都是零。不包含任何信息，意味着完全不确定性，即u = 1。
    - 经过一些训练，让可信度变为 b = < 0.8, 0，…，0 >。这意味着对观点的总信任度（∑bk）是0.8，剩下的0.2是不确定性（u）。狄利克雷强度计算为 S = 10/0.2 = 50。因此，第一类得到的新证据量计算为 50 × 0.8 = 40。在这种情况下，对应狄利克雷分布D(p; < 41,1，…, 1〉)。
- 对于给定一个观点，第k个单例（标签类别）的期望概率是对应的狄利克雷分布的平均值。即最终的预测概率。

![Untitled](./assets/img/posts/20221212/6.png)