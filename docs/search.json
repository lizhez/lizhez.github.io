[
  
    {
      "title"       : "华东师范大学数据科学与工程学院暑期夏令营实践项目总结与反思",
      "category"    : "",
      "tags"        : "学习笔记",
      "url"         : "./huadongshifan.html",
      "date"        : "2023-07-15 00:00:00 +0800",
      "description" : "夏令营项目--个人博客，总结反思",
      "content"     : "在所给的四个项目中，经过仔细的考虑，选择完成【个人博客搭建】这个项目。其原因有一下三点： 首先，在比较了【关联时间序列】和【个人博客】的项目后，由于我未上手搭建过机器学习的模型，因此在14天的时间里是一个巨大的挑战。而本科期间，多项课程设计是有关网站系统的开发，选择【个人博客搭建】更加容易上手。 其次，在我看来【个人博客】是一个很好的展示自己个人能力、兴趣爱好的媒介。而GitHub Pages提供的这种静态网页，可以在没有个人域名和服务器的情况下向外提供个人网站链接，是一个很好的工具。 最后，我一直有过搭建个人博客来记录日常和学习的想法，但平时课程紧张，并没有来得及实现，而这次实践给了我一个合适的机会。目前为止，整个博客构建的较为仓促，后期会对其进行补充和美化，并一直记录。下面我将从5个方面介绍此项目：博客主题的选取 Jekyll框架 Jekyll ，是由 Ruby 编写的、快速、简洁且高效的静态网站生成引擎， 使用一个模板目录作为网站布局的基础框架，并且支持 Markdown、Textile 等标记语言的解析，同时可以直接在 GitHub Pages 上使用 Jekyll来进行个人博客的搭建。 本次项目，选择 Jekyll 正是由于其简易而强大的布局，可以快速上手开始项目搭建。同时 Jekyll Themes 提供了大量的页面模板，可以进行美化。 模板选择 选择 Jekyll theme: Adam Blog 2.0 为模板进行网页的开发。它提供了主页、所有文章、标签、关于我、文章详情5种页面的设计模板，界面简单整洁。 参考链接 源主题地址：Jekyll theme: Adam Blog 2.0 学习教程：B站-基于 GitHub Pages 和 Jekyll 搭建个人博客的简单心得 博客页面布局 PC端 主界面: 上方是页头，包括导航条按钮（默认关闭），博客标题，搜索按钮（可以点击以关键字搜索文章）。下方是文章列表和所有标签。最下方是页脚，包括关键的导航。 导航: 通过点击页头左方三个横线的标志，可以展开导航栏。导航栏上方灯泡标志，控制页面的明亮或黑暗主题。 所有文章：展示所有文章列表，竖向排列。 标签：根据文章标签展示文章。 关于我：个人简单介绍。 文章详情：文章详情页。中间的正文，左侧的标签，而部分页面会显示导航，以及分享的链接按钮，最下方包括其他文章，作者信息，评论。 移动端 博客功能实现 分页 通过 Jekyll 中的分页插件实现主界面中文章的分页功能，这里设置六篇文章为一页。使用 ”jekyll install paginator“ 进行下载。 根据当前页面是否由前一个或后一个进行自动的变化。 搜索 这里可以通过关键字，对文章标题和文章内容进行检索。 标签 除了对文章根据标题进行展示外，同时通过在文章页面的开头使用 “tags: [ 学习笔记]” 为每个文章添加标签。此时可以根据标签对文章进行搜索。这里举例以证据学习查询，可以看到 评论 使用 GitHub APPs 中的 utterances 实现评论功能 # Commentscomments: utteranc #[disqus, utteranc]comments_opts:comments_curtain: yes # leave empty to show the disqus embed directlyrepo: lizhez/lizhez.github.io # The GitHub repo URL. https://utteranc.es/issue_term: title # The GitHub issue labeltheme: github-light # The GitHub comment's there. e.g. github-dark 博客制作过程中遇到的问题 参考资料相对较少 相较于大型 web开发 框架（如 SpringBoot）网上存在大量资源和教程不同，Jekyll由于其轻量、简单的特点，从环境配置安装到使用，网络上系统化的学习资料很少。因此主要是根据实战视频，来了解 Jekyll 框架的结构和功能，并通过实际上手，来观察具体操作会影响到页面上什么内容的改变。 LaTeX公式书写的复杂度 之前在记录学习笔记时，为了简单快速，很多都是直接截屏处理。而此尝试使用 LaTeX公式 的书写，来美化文章页面。由于时间问题，只在其中一个页面 理解狄利克雷 使用。 总结经过此次实践过程，新学习了一项新的技术，同时完成了个人博客的搭建。这不仅仅是一项实践任务，同时也是一项可以实际使用的功能，向外提供了交流和学习的途径。当前博客还有很多不足，计划在后续增加分类、播放等其他功能，继续完善。"
    } ,
  
    {
      "title"       : "等疫情结束就去趟海边吧",
      "category"    : "",
      "tags"        : "旅行, 海边, 摄影",
      "url"         : "./to-qingdao.html",
      "date"        : "2023-04-10 21:32:20 +0800",
      "description" : "我们终将告别黄昏，从此挣脱阴暗，向光里坠落",
      "content"     : "说到海边，我想到的就是阳光、沙滩、椰子，从日出坐到日落。晒晒太阳，吹吹海风，看一盛大日出和日落。但其实，海边也可以是礁石，鲜花和海风。青岛的海是多变的。冬天，他凌冽而广阔，静谧的海面似乎蕴藏着巨大的谜团。春日，他温柔而清凉，海鸥盘旋在海的上空，仿佛一双温柔的手抚平心中的不安与焦虑。看海在海边看着太阳缓缓从天边升起，一层金光逐渐洒向海面，慢慢拂上我的肩膀。清晨的海边是静悄悄的，阳光驱散黑暗的同时也仿佛带走了我们的疲惫。连夜赶路的困倦瞬间消失的无影无踪，这是大海带来的震撼与惊喜。奔跑下午，从大学路到龙江路，向信号山公园走去。大学路的红墙黄瓦，龙江路的卡通墙绘，古风和动漫有了奇妙的结合。一路上车辆很少，我们奔跑在狭窄的马路上，是青春肆意的洒脱。"
    } ,
  
    {
      "title"       : "Evidential Deep Learning",
      "category"    : "",
      "tags"        : "证据学习, 论文笔记",
      "url"         : "./evidential-deep-learning.html",
      "date"        : "2022-12-12 00:00:00 +0800",
      "description" : "论文《Evidential Deep Learning》的阅读笔记",
      "content"     : "问题 由于标准方法是训练网络以使预测损失最小化，模型对其预测置信度一无所知。 已有的方式：通过权重不确定性间接推断预测不确定性的贝叶斯神经网络 提出：使用主观逻辑理论（SL）的显式建模。在类概率上放置狄利克雷分布，将神经网络的预测视为主观意见。学习从数据中收集导致这些意见的确定性神经网络的证据的函数。 Softmax存在的问题：夸大预测类别的概率，结果是不可靠的不确定性估计。（预测的标签的距离除了与其他类别的比较值外，对结论没有用处）理论说明 Subjective Logic (SL)主观逻辑，为每个单例提供 可信度 bk 考虑 K 个互斥单例(类标签)，并提供总体不确定性u。 可信度 bk 由单例的证据 ek 计算。设 ek≥0 为第 k 个单例的证据，可信度 bk 和不确定性 u 计算为 不确定性与总证据成反比 当没有证据时，每个单例的信度为0，不确定性为1。 证据是从数据中收集的支持量的度量，用以支持将样本分类到某个类别。 可信度（主观意见），对应于 参数αk = ek + 1的狄利克雷分布 bk = (αk−1)/S，从相应的狄利克雷分布的参数中得到主观意见，S =∑k αi被称为狄利克雷强度 举例 假设b = &lt; 0，…， 0 &gt;为10类分类问题的可信度分配。则对图像进行分类的先验分布变为均匀分布，即D(p; &lt; 1，…， 1 &gt;) 参数均为1的狄利克雷分布。简单说就是因为没有观察到的证据，所以可信度都是零。不包含任何信息，意味着完全不确定性，即u = 1。 经过一些训练，让可信度变为 b = &lt; 0.8, 0，…，0 &gt;。这意味着对观点的总信任度（∑bk）是0.8，剩下的0.2是不确定性（u）。狄利克雷强度计算为 S = 10/0.2 = 50。因此，第一类得到的新证据量计算为 50 × 0.8 = 40。在这种情况下，对应狄利克雷分布D(p; &lt; 41,1，…, 1〉)。 对于给定一个观点，第k个单例（标签类别）的期望概率是对应的狄利克雷分布的平均值。即最终的预测概率。"
    } ,
  
    {
      "title"       : "理解狄利克雷分布",
      "category"    : "opinion",
      "tags"        : "Dirichlet, Beta, 机器学习, 证据学习",
      "url"         : "./Dirichlet.html",
      "date"        : "2022-12-11 00:00:00 +0800",
      "description" : "如何理解狄利克雷分布",
      "content"     : "知乎：深入理解Beta分布在看一篇有关证据学习的文章时，文中使用了狄利克雷分布，对此很是迷惑，在搜索大量资料后总结了有关狄利克雷的相关知识。希望可以帮助有需要者快速简单理解。本文中主要的论述来自上述链接。Beta分布在了解狄利克雷分布之前，首先需要了解Beta分布，因为狄利克雷分布其本质上就是Beta分布的多元表示。因此本文主要介绍Beta分布，以引出狄利克雷分布。我们都了解二项分布，其中若随机变量 X 付出参数为 n 和 q 的二项分布，则其概率密度函数为\\(\\begin{align}\\begin{split}p(x)=\\begin{pmatrix}n\\\\x\\end{pmatrix}q^x(1-q)^{n-x}\\end{split}\\end{align}\\)如果将其看作是关于 q 的函数，即表示某件事件以概率 q 出现成为了一个事件，其本身也具有了概率。此时某种事件已经可以看作是一个已知的固定事件，即 x,n为常数，我们只是在考虑概率的概率。这时将其表述为一个分布为\\(\\begin{align}\\begin{split}f(q)=kq^a(1-q)^b\\end{split}\\end{align}\\)其中 a,b 是常数，则对于此时的事件所有概率出现总和应当为1。有\\(\\begin{align}\\begin{split}f(q)=kq^a(1-q)^b\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\int_{0}^{1}f(q)dq=\\int_{0}^{1}kq^a(1-q)^bdq=k\\int_{0}^{1}q^a(1-q)^bdq=1\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}k=\\frac{1}{\\int_{0}^{1}q^a(1-q)^bdq}\\ \\end{split}\\end{align}\\)对于Beta分布，通常记为B(a+1,b+1)，这里 +1 我认为只是为了和Gamma函数对应起来更加简洁。\\(\\begin{align}\\begin{split}B(a+1,b+1)=\\int_{0}^{1}q^a(1-q)^bdq, 则\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}k=B(a+1,b+1)^{-1}\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}f(q;a+1,b+1)=B(a+1,b+1)^{-1}q^a(1-q)^b\\end{split}\\end{align}\\)对其进行改造，使得α=a+1,β=b+1，当q=t时得到Beta函数；当q=x时，得到Beta分布函数\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\int_{0}^{1}t^{\\alpha-1}(1-t)^{\\beta-1}dt, 则\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}f(x;\\alpha,\\beta)=B(\\alpha,\\beta)^{-1}x^{\\alpha-1}(1-x)^{\\beta-1}\\end{split}\\end{align}\\)Beta分布和Gamma函数的关系假设向长度为1的桌子上扔一个红球，会落在0到1这个范围内，设这个长度值为 x ，再向桌上扔一个白球，那么这个白球落在红球左边的概率即为 x 。 若一共扔了 n 次白球，其中每一次都是相互独立的，假设落在红球左边的白球数量为 k，那么随机变量 K 服从参数为 n 和 x 的二项分布。而对于 x 则服从 [0,1] 的均匀分布 X~U[0,1]。对于所有的K和x有\\(\\begin{align}\\begin{split}P(K=k)=\\int_{0}^{1}\\begin{pmatrix}n\\\\x\\end{pmatrix}x^k(1-x)^{n-k}dx=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\int_{0}^{1}x^k(1-x)^{n-k}dx\\end{split}\\end{align}\\)另一种丢球方式，一次性将n+1个球丢出，随机选择一个球作为红球。 则任何球被选中的概率为1/n+1。同样，红球左边有0，1，…，n个球的概率也是 1/n+1。（第一个球被选中=左边有0个球，第二个球被选中=左边有1个球，…， 第n+1个球被选中=左边有n个球） 此时有\\(\\begin{align}\\begin{split}P(K=k)=\\int_{0}^{1}\\begin{pmatrix}n\\\\x\\end{pmatrix}x^k(1-x)^{n-k}dx \\\\=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\int_{0}^{1}x^k(1-x)^{n-k}dx=\\frac{1}{n+1}\\\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\int_{0}^{1}x^k(1-x)^{n-k}dx=\\frac{(n-k)!k!}{n!}\\frac{1}{n+1}=\\frac{k!(n-k)!}{(n+1)!}\\\\end{split}\\end{align}\\)而Gamma函数的定义是\\(\\begin{align}\\begin{split}\\Gamma(m)=\\int_{0}^{+\\infty}e^(-x)x^{m-1}dx=(m-1)!\\\\end{split}\\end{align}\\)令 k=α-1,n-k=β-1，即n=a+b-2，可得到\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\int_{0}^{1}t^{\\alpha-1}(1-t)^{\\beta-1}dt=\\frac{(\\alpha-1)!(\\beta-1)!}{(\\alpha+\\beta-1)!}\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)!}\\end{split}\\end{align}\\)狄利克雷分布正如前文所说，狄利克雷分布是多元的Beta分布，有其中参数α常常被用作证据学习中的证据表示。其代表此证据出现的概率，即可信度。"
    } 
  
]
