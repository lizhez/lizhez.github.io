[
  
    {
      "title"       : "华东师范大学数据科学与工程学院暑期夏令营实践项目总结与反思",
      "category"    : "",
      "tags"        : "学习笔记",
      "url"         : "./huadongshifan.html",
      "date"        : "2023-07-15 00:00:00 +0800",
      "description" : "夏令营项目--个人博客，总结反思",
      "content"     : "在所给的四个项目中，经过仔细的考虑，选择完成【个人博客搭建】这个项目。其原因有一下三点： 首先，在比较了【关联时间序列】和【个人博客】的项目后，由于我未上手搭建过机器学习的模型，因此在14天的时间里是一个巨大的挑战。而本科期间，多项课程设计是有关网站系统的开发，选择【个人博客搭建】更加容易上手。 其次，在我看来【个人博客】是一个很好的展示自己个人能力、兴趣爱好的媒介。而GitHub Pages提供的这种静态网页，可以在没有个人域名和服务器的情况下向外提供个人网站链接，是一个很好的工具。 最后，我一直有过搭建个人博客来记录日常和学习的想法，但平时课程紧张，并没有来得及实现，而这次实践给了我一个合适的机会。目前为止，整个博客构建的较为仓促，后期会对其进行补充和美化，并一直记录。下面我将从5个方面介绍此项目：博客主题的选取 Jekyll框架 Jekyll ，是由 Ruby 编写的、快速、简洁且高效的静态网站生成引擎， 使用一个模板目录作为网站布局的基础框架，并且支持 Markdown、Textile 等标记语言的解析，同时可以直接在 GitHub Pages 上使用 Jekyll来进行个人博客的搭建。 本次项目，选择 Jekyll 正是由于其简易而强大的布局，可以快速上手开始项目搭建。同时 Jekyll Themes 提供了大量的页面模板，可以进行美化。 模板选择 选择 Jekyll theme: Adam Blog 2.0 为模板进行网页的开发。它提供了主页、所有文章、标签、关于我、文章详情5种页面的设计模板，界面简单整洁。 参考链接 源主题地址：Jekyll theme: Adam Blog 2.0 学习教程：B站-基于 GitHub Pages 和 Jekyll 搭建个人博客的简单心得 博客页面布局 PC端 主界面: 上方是页头，包括导航条按钮（默认关闭），博客标题，搜索按钮（可以点击以关键字搜索文章）。下方是文章列表和所有标签。最下方是页脚，包括关键的导航。 导航: 通过点击页头左方三个横线的标志，可以展开导航栏。导航栏上方灯泡标志，控制页面的明亮或黑暗主题。 所有文章：展示所有文章列表，竖向排列。 标签：根据文章标签展示文章。 关于我：个人简单介绍。 文章详情：文章详情页。中间的正文，左侧的标签，而部分页面会显示导航，以及分享的链接按钮，最下方包括其他文章，作者信息，评论。 移动端博客功能实现博客制作过程中遇到的问题总结经过此次实践过程，"
    } ,
  
    {
      "title"       : "等疫情结束就去趟海边吧",
      "category"    : "",
      "tags"        : "旅行, 海边, 摄影",
      "url"         : "./to-qingdao.html",
      "date"        : "2023-04-10 21:32:20 +0800",
      "description" : "我们终将告别黄昏，从此挣脱阴暗，向光里坠落",
      "content"     : "说到海边，我想到的就是阳光、沙滩、椰子，从日出坐到日落。晒晒太阳，吹吹海风，看一盛大日出和日落。但其实，海边也可以是礁石，鲜花和海风。青岛的海是多变的。冬天，他凌冽而广阔，静谧的海面似乎蕴藏着巨大的谜团。春日，他温柔而清凉，海鸥盘旋在海的上空，仿佛一双温柔的手抚平心中的不安与焦虑。看海在海边看着太阳缓缓从天边升起，一层金光逐渐洒向海面，慢慢拂上我的肩膀。清晨的海边是静悄悄的，阳光驱散黑暗的同时也仿佛带走了我们的疲惫。连夜赶路的困倦瞬间消失的无影无踪，这是大海带来的震撼与惊喜。奔跑下午，从大学路到龙江路，向信号山公园走去。大学路的红墙黄瓦，龙江路的卡通墙绘，古风和动漫有了奇妙的结合。一路上车辆很少，我们奔跑在狭窄的马路上，是青春肆意的洒脱。"
    } ,
  
    {
      "title"       : "Evidential Deep Learning",
      "category"    : "",
      "tags"        : "证据学习, 论文笔记",
      "url"         : "./evidential-deep-learning.html",
      "date"        : "2022-12-12 00:00:00 +0800",
      "description" : "论文《Evidential Deep Learning》的阅读笔记",
      "content"     : "问题 由于标准方法是训练网络以使预测损失最小化，模型对其预测置信度一无所知。 已有的方式：通过权重不确定性间接推断预测不确定性的贝叶斯神经网络 提出：使用主观逻辑理论（SL）的显式建模。在类概率上放置狄利克雷分布，将神经网络的预测视为主观意见。学习从数据中收集导致这些意见的确定性神经网络的证据的函数。 Softmax存在的问题：夸大预测类别的概率，结果是不可靠的不确定性估计。（预测的标签的距离除了与其他类别的比较值外，对结论没有用处）理论说明 Subjective Logic (SL)主观逻辑，为每个单例提供 可信度 bk 考虑 K 个互斥单例(类标签)，并提供总体不确定性u。 可信度 bk 由单例的证据 ek 计算。设 ek≥0 为第 k 个单例的证据，可信度 bk 和不确定性 u 计算为 不确定性与总证据成反比 当没有证据时，每个单例的信度为0，不确定性为1。 证据是从数据中收集的支持量的度量，用以支持将样本分类到某个类别。 可信度（主观意见），对应于 参数αk = ek + 1的狄利克雷分布 bk = (αk−1)/S，从相应的狄利克雷分布的参数中得到主观意见，S =∑k αi被称为狄利克雷强度 举例 假设b = &lt; 0，…， 0 &gt;为10类分类问题的可信度分配。则对图像进行分类的先验分布变为均匀分布，即D(p; &lt; 1，…， 1 &gt;) 参数均为1的狄利克雷分布。简单说就是因为没有观察到的证据，所以可信度都是零。不包含任何信息，意味着完全不确定性，即u = 1。 经过一些训练，让可信度变为 b = &lt; 0.8, 0，…，0 &gt;。这意味着对观点的总信任度（∑bk）是0.8，剩下的0.2是不确定性（u）。狄利克雷强度计算为 S = 10/0.2 = 50。因此，第一类得到的新证据量计算为 50 × 0.8 = 40。在这种情况下，对应狄利克雷分布D(p; &lt; 41,1，…, 1〉)。 对于给定一个观点，第k个单例（标签类别）的期望概率是对应的狄利克雷分布的平均值。即最终的预测概率。"
    } ,
  
    {
      "title"       : "理解狄利克雷分布",
      "category"    : "opinion",
      "tags"        : "Dirichlet, Beta, 机器学习, 证据学习",
      "url"         : "./Dirichlet.html",
      "date"        : "2022-12-11 00:00:00 +0800",
      "description" : "如何理解狄利克雷分布",
      "content"     : "知乎：深入理解Beta分布在看一篇有关证据学习的文章时，文中使用了狄利克雷分布，对此很是迷惑，在搜索大量资料后总结了有关狄利克雷的相关知识。希望可以帮助有需要者快速简单理解。本文中主要的论述来自上述链接。Beta分布在了解狄利克雷分布之前，首先需要了解Beta分布，因为狄利克雷分布其本质上就是Beta分布的多元表示。因此本文主要介绍Beta分布，以引出狄利克雷分布。我们都了解二项分布，其中若随机变量 X 付出参数为 n 和 q 的二项分布，则其概率密度函数为\\(\\begin{align}\\begin{split}p(x)=\\begin{pmatrix}n\\\\x\\end{pmatrix}q^x(1-q)^{n-x}\\end{split}\\end{align}\\)如果将其看作是关于 q 的函数，即表示某件事件以概率 q 出现成为了一个事件，其本身也具有了概率。此时某种事件已经可以看作是一个已知的固定事件，即 x,n为常数，我们只是在考虑概率的概率。这时将其表述为一个分布为\\(\\begin{align}\\begin{split}f(q)=kq^a(1-q)^b\\end{split}\\end{align}\\)其中 a,b 是常数，则对于此时的事件所有概率出现总和应当为1。有\\(\\begin{align}\\begin{split}f(q)=kq^a(1-q)^b\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\int_{0}^{1}f(q)dq=\\int_{0}^{1}kq^a(1-q)^bdq=k\\int_{0}^{1}q^a(1-q)^bdq=1\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}k=\\frac{1}{\\int_{0}^{1}q^a(1-q)^bdq}\\ \\end{split}\\end{align}\\)对于Beta分布，通常记为B(a+1,b+1)，这里 +1 我认为只是为了和Gamma函数对应起来更加简洁。\\(\\begin{align}\\begin{split}B(a+1,b+1)=\\int_{0}^{1}q^a(1-q)^bdq, 则\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}k=B(a+1,b+1)^{-1}\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}f(q;a+1,b+1)=B(a+1,b+1)^{-1}q^a(1-q)^b\\end{split}\\end{align}\\)对其进行改造，使得α=a+1,β=b+1，当q=t时得到Beta函数；当q=x时，得到Beta分布函数\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\int_{0}^{1}t^{\\alpha-1}(1-t)^{\\beta-1}dt, 则\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}f(x;\\alpha,\\beta)=B(\\alpha,\\beta)^{-1}x^{\\alpha-1}(1-x)^{\\beta-1}\\end{split}\\end{align}\\)Beta分布和Gamma函数的关系假设向长度为1的桌子上扔一个红球，会落在0到1这个范围内，设这个长度值为 x ，再向桌上扔一个白球，那么这个白球落在红球左边的概率即为 x 。 若一共扔了 n 次白球，其中每一次都是相互独立的，假设落在红球左边的白球数量为 k，那么随机变量 K 服从参数为 n 和 x 的二项分布。而对于 x 则服从 [0,1] 的均匀分布 X~U[0,1]。对于所有的K和x有\\(\\begin{align}\\begin{split}P(K=k)=\\int_{0}^{1}\\begin{pmatrix}n\\\\x\\end{pmatrix}x^k(1-x)^{n-k}dx=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\int_{0}^{1}x^k(1-x)^{n-k}dx\\end{split}\\end{align}\\)另一种丢球方式，一次性将n+1个球丢出，随机选择一个球作为红球。 则任何球被选中的概率为1/n+1。同样，红球左边有0，1，…，n个球的概率也是 1/n+1。（第一个球被选中=左边有0个球，第二个球被选中=左边有1个球，…， 第n+1个球被选中=左边有n个球） 此时有\\(\\begin{align}\\begin{split}P(K=k)=\\int_{0}^{1}\\begin{pmatrix}n\\\\x\\end{pmatrix}x^k(1-x)^{n-k}dx \\\\=\\begin{pmatrix}n\\\\x\\end{pmatrix}\\int_{0}^{1}x^k(1-x)^{n-k}dx=\\frac{1}{n+1}\\\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}\\int_{0}^{1}x^k(1-x)^{n-k}dx=\\frac{(n-k)!k!}{n!}\\frac{1}{n+1}=\\frac{k!(n-k)!}{(n+1)!}\\\\end{split}\\end{align}\\)而Gamma函数的定义是\\(\\begin{align}\\begin{split}\\Gamma(m)=\\int_{0}^{+\\infty}e^(-x)x^{m-1}dx=(m-1)!\\\\end{split}\\end{align}\\)令 k=α-1,n-k=β-1，即n=a+b-2，可得到\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\int_{0}^{1}t^{\\alpha-1}(1-t)^{\\beta-1}dt=\\frac{(\\alpha-1)!(\\beta-1)!}{(\\alpha+\\beta-1)!}\\end{split}\\end{align}\\)\\(\\begin{align}\\begin{split}B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)!}\\end{split}\\end{align}\\)狄利克雷分布正如前文所说，狄利克雷分布是多元的Beta分布，有其中参数α常常被用作证据学习中的证据表示。其代表此证据出现的概率，即可信度。"
    } 
  
]
